A hash table is a data structure that stores key-value pairs using a hash function.

Best case (O(1)) â†’ If there are no collisions, inserting, deleting, and searching take constant time.
Worst case (O(n)) â†’ If too many collisions happen, all elements may go into one bucket, making it behave like a linked list.
Average case (O(1)) â†’ With a good hash function and proper handling of collisions, operations remain fast.
space O(n) all the case  

Rehashing is the process of resizing a hash table when it becomes too full. It involves:
Creating a new, larger hash table.
Recomputing the hash values for all existing keys.
Inserting them into the new table.
This prevents excessive collisions and maintains efficient lookup time.


How Stack is Used in Undo-Redo Operations
A stack is a LIFO (Last In, First Out) data structure, meaning the last action is undone first.
Undo: When an action is performed, itâ€™s pushed onto the "undo" stack.
Redo: When an action is undone, it is pushed onto the "redo" stack.
If a new action is performed, the redo stack is cleared.

Hash Table Complexity with Reason
Insertion/Search/Deletion: Average O(1) because a good hash function spreads keys evenly.
Worst case O(n): Happens when all elements collide and form a long chain.

A bounded queue is a queue with a fixed size.
If the queue is full, new elements cannot be added until some are removed.
Used in thread pools, buffer management, etc.

Load factor is crucial for hash table efficiency.
Ideal range: 0.5 â€“ 0.7.
If >0.7, rehashing is needed.
If too low, memory is wasted.
The load factor measures how full a hash table is. It is calculated as:
LoadÂ Factor = NumberÂ ofÂ ElementsÂ inÂ HashÂ Table / TotalÂ BucketsÂ inÂ HashÂ Table
If the load factor is too high, there are too many elements in the table, leading to more collisions (when multiple elements try to occupy the same bucket). This makes searches and insertions slower.
If the load factor is too low, many buckets remain empty, wasting memory.
Why Does Rehashing Happen?
To maintain good performance, most hash tables resize (rehash) when the load factor goes beyond 0.7 (or another threshold).
A new, larger hash table is created (usually double the size).
All existing elements are reinserted into the new table using the new hash function.
This reduces collisions and speeds up operations.

What is a Priority Queue?
A priority queue is like a regular queue, but instead of following the first-in, first-out (FIFO) rule, it removes elements based on their priority.
How Does It Work?
Each element in the queue has a priority value.
When removing an element (dequeue), the highest-priority element is removed first, not necessarily the one that arrived first.
Implemented using heaps for efficiency.
Used in Dijkstraâ€™s algorithm, CPU scheduling, etc.

Double-Ended Queue (Deque)
A deque (double-ended queue) allows insertion and deletion from both ends.
Types:
Input-restricted deque: Insertion allowed at one end, deletion from both.
Output-restricted deque: Deletion allowed at one end, insertion from both.
Used in browsers (back/forward), task scheduling, sliding window problems.
Applications: Used in sliding window problems, scheduling tasks, etc.

Clustering in Linear Probing
In linear probing, when a collision happens, elements get placed one after another (back-to-back). This forms clusters
Quadratic Probing
When a collision happens in a hash table, quadratic probing helps find the next available spot by jumping in increasing gaps instead of checking the next bucket one by one.

A HashSet is a collection of unique elements, implemented using a hash table.
Operations (add, remove, contains) are on average O(1).
No duplicates allowed.
Used for fast lookups.

In-Place Sort
A sorting algorithm is in-place if it uses a small, constant amount of extra memory.
Examples: Bubble Sort, Quick Sort, Insertion Sort.
Not in-place: Merge Sort (uses extra space).

Probing is a method used to find an empty slot in a hash table when a collision occurs.
Linear Probing: Check the next slot.
Quadratic Probing: Check in a quadratic pattern.
Double Hashing: Use a second hash function to jump around.

The stack pointer (SP) is a special register that stores the address of the top of the stack.
Used in function calls, recursion, and memory management.
When a function is called, its local variables are stored in the stack, and the stack pointer updates.

A collision resolution technique where all elements are stored within the hash table itself.

////////////////OPEN ADDRESSING/////////////////
No separate linked lists.
Uses probing to find an empty slot.
Methods: Linear Probing, Quadratic Probing, Double Hashing.

Feature	                        Quadratic Probing ðŸ”µ	                                Double Hashing ðŸ”´
How It Resolves Collisions	    Moves in a quadratic sequence (1Â², 2Â², 3Â², ...)	        Uses a second hash function to determine step size
Formula	                        index = (h1(key) + iÂ²) % table_size	                    index = (h1(key) + i * h2(key)) % table_size
Clustering	                    Can still cause secondary clustering	                Avoids clustering better
Performance	                    Works well in smaller tables	                        Works well for larger tables
Flexibility	                    Jump pattern is fixed	                                Jump pattern varies based on h2(key)


Types of Hash Functions
A hash function converts a key into an index in a hash table.
Division Method: h(key) = key % table_size (Simple but not always efficient).
Multiplication Method: h(key) = floor(table_size * (key * A % 1)) (Avoids clustering).
Mid-Square Method: Square the key and take the middle digits.
Folding Method: Break the key into parts, sum them, and take modulo.
Universal Hashing: Use a random function to reduce worst-case attacks.


Disadvantages of Quicksort over Merge Sort
QuickSort	                                        MergeSort

Unstable sorting â€“ Doesn't preserve 
the order of equal elements.	                    Stable sorting â€“ Maintains the order of equal elements.

Worst-case time complexity O(nÂ²) â€“ If the
pivot is not chosen well, performance degrades.	    Consistent performance â€“ Always O(n log n), even in the worst case.

Not ideal for linked lists â€“ Relies on random 
access for performance, which is slower 
with linked lists.	                                Better for linked lists â€“ Since it works by dividing and merging, it is faster on linked lists.

In-place sorting â€“ Requires fewer memory allocations
but may cause stack overflow for large datasets.	Extra memory needed â€“ Needs additional space for merging, making it less memory efficient.


Yes, Merge Sort has a higher space complexity compared to QuickSort:
Merge Sort: O(n) because it requires additional space to merge the subarrays.
QuickSort: O(log n) in the best case (for the recursion stack), as it sorts in place.

How to Handle Collisions in a Linked List (Hash Table)
Chaining: Store multiple elements in a linked list at the same index. If two keys hash to the same value, they are stored as nodes in the linked list at that index. When searching, the list is traversed.

Linked lists are useful because:
Dynamic Size
Efficient Insertion / Deletion
Memory Efficient


Importance of Pivot Value in QuickSort
The pivot in QuickSort determines how the array is partitioned into two subarrays. Its value is crucial because:
It should divide the array as evenly as possible.
A bad pivot choice leads to an unbalanced partition, leading to O(nÂ²) time complexity.
Choosing the median of three as a pivot improves performance and avoids worst-case behavior.


QuickSort vs MergeSort (4 Points Each)
QuickSort	                                                                MergeSort

In-place sorting â€“ Sorts elements in the same array
without extra space.	                                                    Requires extra space â€“ O(n) space needed to merge subarrays.

Unstable â€“ Does not preserve the order of equal elements.	                Stable â€“ Preserves the order of equal elements.

Worst-case time complexity O(nÂ²) â€“ If pivot selection is poor.	            Guaranteed O(n log n) in all cases.

Faster on average â€“ Performs faster for smaller arrays and average cases.	Better for large datasets â€“ Stable and consistent performance even in the worst case.

How to Handle Collision in a Linked List (for Hash Tables)
In a hash table that uses a linked list to handle collisions, the idea is to store multiple elements (that hash to the same index) in a linked list at the same position.
If two or more elements hash to the same index, they are added to a linked list that begins at that index.
Each node in the linked list stores a key-value pair.

Linear Probing: Check the next available slot.
Quadratic Probing: Check in a quadratic pattern.
Double Hashing: Use a second hash function.

A stable sort keeps the relative order of elements that have equal values.
Example: If two employees have the same salary, a stable sort will keep their relative order the same as before sorting.
Examples of stable sorts: MergeSort, Insertion Sort.
Unstable sorts: QuickSort (by default).

Stack underflow happens when you try to pop or peek from an empty stack.
Itâ€™s an error that occurs because the stack has no items to remove.

Applications of Hash Tables
Focus on understanding hash tables as used in:
Database indexing for fast lookups.
Caches for frequently accessed data.
Sets and maps for data storage.
Unique key-value pair mapping in various applications (e.g., dictionaries, unique elements).

Applications of Hash Tables
1. Fast Lookups â€” Searching for an item in constant time O(1), such as in a dictionary or database.
2. Caching â€” Hash tables are used to store frequently accessed data in cache systems (e.g., LRU cache).
3. Sets and Maps â€” Implementation of sets (for unique elements) and maps (key-value pairs) in many programming languages.
4. Database Indexing â€” Hash tables are used to quickly access rows based on primary keys.

Applications of Queues
1. Task Scheduling â€” Queues are used to schedule tasks in operating systems (e.g., CPU task scheduling).
2. Printing Jobs â€” Printing tasks are handled in a queue to ensure the correct order.
3. BFS Traversal â€” Breadth-First Search (BFS) uses a queue to explore nodes level by level.
4. Event-driven Simulation â€” Queues model processes in event-driven simulations (e.g., network packet processing).

Advantage of MergeSort Over QuickSort (4 Points)
MergeSort	                                                                           QuickSort
1. Always O(n log n) time complexity â€” More predictable performance.	              1. Worst-case time complexity O(nÂ²) â€” Poor pivot selection can degrade performance.
2. Stable Sort â€” Maintains relative order of equal elements.	                        2. Not stable â€” Can change the order of equal elements.
3. Better for large datasets â€” MergeSort handles large data better
because of consistent performance.	                                                    3. May struggle with recursion depth â€” QuickSort can lead to                                                                                    stack overflow in   case of deep recursion.
4. Suitable for linked lists â€” Can be implemented without extra space for linked lists.	4. Inefficient for large datasets â€” QuickSort may degrade when the pivot choice is poor.

Applications of Stack
Backtracking: Used in solving problems like mazes or puzzles where you need to remember previous steps.
Expression Parsing: Used in evaluating expressions like in postfix or infix notation.
Function Call Stack: Manages function calls in recursion.


Disadvantages of MergeSort (4 Points)
MergeSort Disadvantages
1. Space complexity O(n) â€” Requires extra space for merging.
2. Slower for small datasets â€” MergeSort has overhead for small datasets compared to simpler algorithms like Insertion Sort.
3. Slower compared to QuickSort in practice â€” Due to extra space and merging overhead.
4. Not in-place â€” Additional space is needed for merging subarrays.

Separate chaining is a technique used to handle collisions in hash tables. Instead of storing a single element at each hash table index, each index stores a linked list (or a bucket), where all elements that hash to the same index are stored.
How it works: When two keys hash to the same index, they are placed into a linked list at that index. When searching for a key, the list is traversed until the correct key is found.
Advantages:
Can store an unlimited number of elements at each index.
Simple to implement.
Disadvantages:
Requires extra space due to the linked list.
Performance degrades if the hash function isn't well distributed, as it can cause longer lists.

Hashing is a technique used to convert input data (like keys) into a fixed-size value (called a hash) that acts as an index into a data structure such as a hash table. This index is used to quickly locate and access the corresponding data.

Hash Table and Collision Resolution
A hash table is a data structure that maps keys to values for efficient retrieval. When a hash function results in the same index for multiple keys (collision), we need to use a collision resolution method, such as:
Separate Chaining: Store a list or another data structure at each index.
Linear Probing: Search sequentially for the next available slot.
Quadratic Probing: Search for the next available slot using a quadratic function.
Double Hashing: Use a second hash function to calculate the new index if a collision occurs.


Queues are linear data structures used to store elements in a FIFO (First In, First Out) order. The most common types of queues are:
Simple Queue: The standard FIFO queue where elements are added to the rear and removed from the front.
Circular Queue: A queue where the front and rear pointers are connected, allowing efficient use of space by wrapping around.
Priority Queue: A queue where elements are ordered by priority, and the element with the highest priority is dequeued first.
Deque (Double-Ended Queue): A queue where elements can be added or removed from both ends (front and rear).
Double-ended Priority Queue: A more advanced version of the priority queue where you can add or remove elements with the highest or lowest priority.


Hashtable vs HashSet (4 Points)
Hashtable	            HashSet
1. Key-Value Pair â€” Stores pairs of keys and values.	1. Only Keys â€” Stores only unique keys, no values.
2. Allows Nulls â€” Allows null keys and values (depending on the implementation).	2. No Null Elements â€” Does not allow null elements (or at least doesnâ€™t store them).
4. Slower Operations â€” Due to synchronization overhead, operations may be slower than HashSet.  4. Faster Operations â€” HashSet is typically faster as it is not synchronized.


Does Pivot Affect Performance? (in QuickSort)
Yes, the pivot choice in QuickSort greatly affects its performance. A good pivot divides the array into two equal parts, leading to an optimal time complexity of O(n log n). A bad pivot, such as the smallest or largest element, results in unbalanced partitions, causing the time complexity to degrade to O(nÂ²).
Best-case: Pivot divides the array evenly.
Worst-case: Pivot is the smallest or largest element, resulting in highly unbalanced partitions.

Monotonic Stack
A monotonic stack is a stack used to maintain elements in a monotonic (either increasing or decreasing) order. It helps in solving problems where you need to track the minimum or maximum elements in a range or process elements based on their order.
How It Works:
For a monotonically increasing stack, the stack keeps track of elements in increasing order.
For a monotonically decreasing stack, the stack keeps track of elements in decreasing order.
Applications:
Next Greater Element: Finding the next greater element for each element in an array.
Stock Span Problem: Finding the number of consecutive days the stock price was lower than the current day's price.

Monotonic Queue
A monotonic queue is similar to a monotonic stack, but it maintains its elements in either increasing or decreasing order while supporting queue operations like enqueue and dequeue.
How It Works:
For an increasing monotonic queue, each new element is inserted while maintaining the order. If a new element is greater than the back of the queue, elements are dequeued until the condition is satisfied.
For a decreasing monotonic queue, it works similarly, but the queue keeps elements in decreasing order.
Applications:
Sliding Window Maximum: Helps in finding the maximum element in a sliding window of an array in linear time.
Moving Average: Used in streaming data applications to calculate the moving average in real-time.

A bucket in a hash table refers to the collection of all elements that hash to the same index. In separate chaining, each bucket typically stores a linked list or another data structure. Buckets help manage collisions in a hash table by grouping all colliding elements together.
As the number of collisions grows, the number of elements in a bucket may grow too, causing the hash table to resize.

Chaining is a method used for collision resolution in hash tables. When multiple keys hash to the same index, they are stored in a linked list (or other collections like a dynamic array) at that index.
Advantages:
No clustering: Avoids clustering issues like in linear probing.
Simple implementation.
Dynamic growth: No fixed size for the number of elements at each index.


stable sorting means it maintains the order of the same 2 elements if both have the same value instead of swapping them unnecessarily.
merge bubble insertion

Hashing and encryption are both methods used to protect data, but they serve different purposes:
Hashing: A one-way transformation that maps data to a fixed-length value (hash). Itâ€™s used for data integrity, password storage, etc. Irreversible.
eg: SHA-1 (Secure Hash Algorithm 1) is a cryptographic hash function that produces a 160-bit hash value from any input. Itâ€™s fast and generates a 
fixed-size hash, but is now considered insecure due to collision vulnerabilities. Safer alternatives like SHA-256 or SHA-3 are recommended for modern applications.
CRC32 (Cyclic Redundancy Check 32-bit) is an error-detecting code used to check data integrity in networks and storage. It produces a 32-bit hash and is very fast, but it's not suitable for cryptographic security due to its vulnerability to intentional tampering.
MD5 (Message Digest Algorithm 5) produces a 128-bit hash value and is widely used for checksums and data integrity. However, itâ€™s vulnerable to collision attacks, making it unsuitable for modern security applications.
Encryption: A two-way transformation that converts data into a form that can only be reversed with a key. Itâ€™s used for confidentiality.

Data Structure applications :- Arrays,Linked Lists,Stacks,Queue,HashTable,Trees,Graph


Selection Sort picks the minimum and swaps it into place.
Insertion Sort inserts each element into the correct position in a growing sorted list.
Selection Sort is easy to implement and in-place but is generally inefficient and unstable.
Merge Sort provides consistent O(n log n) performance and stability but requires additional space.
Quick Sort is often the fastest in practice, though it can degrade to O(nÂ²) in the worst case and is not stable by default.


Separate chaining is a technique used to handle collisions in a hash table. When two or more keys hash to the same index, instead of replacing the existing value, the hash table stores multiple values at that index using a data structure like a linked list.
âš¡ Pros:
Simple to implement.
No limit on the number of elements in a bucket (depending on the linked list).
âš¡ Cons:
Wastes space if few collisions.
Performance degrades to O(n) in the worst case (if many elements hash to the same index).